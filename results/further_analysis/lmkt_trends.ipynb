{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this analysis is to observe trends across question difficulty as assess by our LMKT model. Specifically, we want to see what types of questions do students differ in difficulty with respect to, and what types of question are consistently easy or hard. Does this align with our knowledge of difficult language learning concepts? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to study this, we first load training data for our question generation model, which as our paper describes, contains difficulty values predicted by our LMKT models for the last question in the sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "with open(\"../../data/generation_train_data/french_train\",\"rb\") as f:\n",
    "    french_data = f.readlines()\n",
    "with open(\"../../data/generation_train_data/spanish_train\",\"rb\") as f:\n",
    "    spanish_data = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now build dictionaries mapping the different target questions to their estimated difficulty for different students. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_tuple(data_line):\n",
    "    data_line = str(data_line)\n",
    "    difficulty = float(data_line.split(\"<QU>\")[0][8:])\n",
    "    student = \"<QU>\".join(data_line.split(\"<G>\")[0].split(\"<QU>\")[1:])\n",
    "    student = hashlib.md5(student.encode()).hexdigest()\n",
    "    target = data_line.split(\"<G>\")[-1][:-9]\n",
    "    return student, difficulty, target\n",
    "\n",
    "def build_dict(data):\n",
    "    data_dict = defaultdict(list)\n",
    "    seen_students = defaultdict(list)\n",
    "    for l in data:\n",
    "        student, difficulty, target = get_data_tuple(l)\n",
    "        if(student in seen_students[target]):\n",
    "            continue\n",
    "        seen_students[target].append(student)\n",
    "        data_dict[target].append(difficulty)\n",
    "    return data_dict\n",
    "\n",
    "french_dict = build_dict(french_data)\n",
    "spanish_dict = build_dict(spanish_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can analyze the average difficulty for different target questions across students, as well as the variance in difficulty (std across students), indicating what questions our LM-KT model tends to always deem \"difficult\" or \"easy\", or what questions rely heavily on the specific student knowledge state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " no!\n",
      "0.9651052631578946 0.02255028238087193\n",
      " the man\n",
      "0.8512666666666667 0.09027055629236665\n",
      " we eat nineteen crepes.\n",
      "0.27428125000000003 0.12117864765889037\n",
      " happy new year!\n",
      "0.633547619047619 0.14932645642875597\n",
      " agreed, thank you very much!\n",
      "0.006076923076923079 0.0039117483492588055\n",
      " tonight, we are eating outside.\n",
      "0.24366666666666667 0.011897712198383165\n"
     ]
    }
   ],
   "source": [
    "phrases = [\" no!\", \" the man\", \" we eat nineteen crepes.\", \" happy new year!\", \" agreed, thank you very much!\", \" tonight, we are eating outside.\"]\n",
    "for phrase in phrases:\n",
    "    print(phrase)\n",
    "    print((np.mean(french_dict[phrase])), np.std(french_dict[phrase]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " no, thanks.\n",
      "0.9565999999999998 0.010104784345381462\n",
      " in the kitchen\n",
      "0.8918372093023257 0.06505397767098225\n",
      " a lemon\n",
      "0.8304 0.12645489314376096\n",
      " mom, come in, please.\n",
      "0.6554727272727272 0.15391129139811252\n",
      " why don't you touch the turtle?\n",
      "0.10790909090909091 0.0936438655423782\n",
      " we eat fish at lunchtime.\n",
      "0.06927272727272728 0.036514233610995414\n"
     ]
    }
   ],
   "source": [
    "phrases = [\" no, thanks.\", \" in the kitchen\", \" a lemon\", \" mom, come in, please.\", \" why don't you touch the turtle?\", \" we eat fish at lunchtime.\"]\n",
    "for phrase in phrases:\n",
    "    print(phrase)\n",
    "    print((np.mean(spanish_dict[phrase])), np.std(spanish_dict[phrase]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
